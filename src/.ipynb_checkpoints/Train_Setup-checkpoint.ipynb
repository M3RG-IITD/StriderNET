{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import scipy.stats as st\n",
    "from jax.config import config  \n",
    "config.update('jax_enable_x64', True)\n",
    "from models import Pol_Net, Policy_Net\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random,jit,lax\n",
    "import jax\n",
    "from jax import ops\n",
    "from jax.example_libraries import optimizers\n",
    "from jax.lax import fori_loop\n",
    "import jraph\n",
    "import time\n",
    "\n",
    "import jax_md\n",
    "from jax_md import space, smap, energy, minimize, quantity, simulate\n",
    "from typing import Any, NamedTuple,  Optional, Union\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from jax._src import prng\n",
    "Array = Any\n",
    "KeyArray = Union[Array, prng.PRNGKeyArray]\n",
    "import timeit\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Debugging\n",
    "from Systems import lennard_jones, SW_Silicon, CSH\n",
    "import Systems\n",
    "from Generic_system import Generic_system\n",
    "from Utils import *\n",
    "from Optimizers import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_index(elem,arr):\n",
    "    return jnp.where(elem==arr,size=1)[0][0]\n",
    "get_indeces_fn=jax.jit(jax.vmap(get_index,in_axes=(0,None)))\n",
    "\n",
    "\n",
    "def Batch_choose_topK_e_greedy(Mux_Muy,probs,key,B_sz,K=1,epsilon=1.0):#eps= 1.0 during training\n",
    "    \"\"\"Chooses n-nodes*K nodes , K from each graph, after renormalizing the output probabilities\n",
    "    \"\"\"\n",
    "    N=int(Mux_Muy.shape[0]/B_sz)\n",
    "    node_indeces=onp.zeros((B_sz,K),dtype='int32')\n",
    "    node_probs=jnp.zeros((B_sz,K))\n",
    "    #choosen_Mu_vec=jnp.zeros((B_sz,N,2))\n",
    "    key1, key2 = random.split(key, 2)\n",
    "    keys=random.split(key2,B_sz)\n",
    "    sample_p = random.uniform(key1,(B_sz,))\n",
    "    for p in range(len(sample_p)):\n",
    "        myprobs=probs[p*N:(p+1)*N]\n",
    "        myprobs=myprobs*(1/jnp.sum(myprobs))\n",
    "        if(sample_p[p]<epsilon):\n",
    "            #Greedy\n",
    "            random_choice=random.choice(keys[p],myprobs,shape=(K,1),replace=False,p=myprobs.reshape(-1))\n",
    "        else:\n",
    "            #Random\n",
    "            random_choice=random.choice(keys[p],myprobs,shape=(K,1),replace=False)  \n",
    "        ind=get_indeces_fn(random_choice,myprobs)\n",
    "        node_indeces[p,:]=ind\n",
    "        node_probs=node_probs.at[p,:].set(myprobs[ind].reshape(-1))\n",
    "        #choosen_Mu_vec=choosen_Mu_vec.at[p,ind].set(Mux_Muy[p*N+ind])\n",
    "    return node_indeces, node_probs\n",
    "\n",
    "#Disp Function\n",
    "\n",
    "           #Disp Function\n",
    "\n",
    "def pdf_multivariate_gauss(x, mu, cov):\n",
    "    \"\"\"Removed part1 for scaling reason[w/0 part1 it is in 0 to 1]: pdf gives density not probability\"\"\"\n",
    "   \n",
    "    '''\n",
    "    Caculate the multivariate normal density (pdf)\n",
    "    \n",
    "    Keyword arguments:\n",
    "        x = numpy array of a \"d x 1\" sample vector\n",
    "        mu = numpy array of a \"d x 1\" mean vector\n",
    "        cov = \"numpy array of a d x d\" covariance matrix\n",
    "    '''\n",
    "    #assert(mu.shape[0] > mu.shape[1]), 'mu must be a row vector'\n",
    "    #assert(x.shape[0] > x.shape[1]), 'x must be a row vector'\n",
    "    #assert(cov.shape[0] == cov.shape[1]), 'covariance matrix must be square'\n",
    "    #assert(mu.shape[0] == cov.shape[0]), 'cov_mat and mu_vec must have the same dimensions'\n",
    "    #assert(mu.shape[0] == x.shape[0]), 'mu and x must have the same dimensions'\n",
    "    #part1 = 1 / ( ((2* jnp.pi)**(len(mu)/2)) * (jnp.linalg.det(cov)**(1/2)) )\n",
    "    part2 = (-1/2) * ((x-mu).T.dot(jnp.linalg.inv(cov))).dot((x-mu))\n",
    "    #return part1 * jnp.exp(part2)\n",
    "    return jnp.exp(part2)\n",
    "\n",
    "vmap_pdf_multivariate_gauss=jax.jit(jax.vmap(jax.vmap(pdf_multivariate_gauss)))\n",
    "\n",
    "def Batch_pred_disp_vec(Mu,key,B_sz=1,std=0.01,spatial_dim=3):\n",
    "    mean = Mu\n",
    "    K=mean.shape[1]\n",
    "    cov = jnp.array([jnp.eye(spatial_dim)*(std**2)]*B_sz*K).reshape((B_sz,K,spatial_dim,spatial_dim))\n",
    "    Pred_disp= jax.random.multivariate_normal(key,mean,cov)\n",
    "    probs=vmap_pdf_multivariate_gauss(Pred_disp,mean,cov)\n",
    "    return Pred_disp, jnp.log(probs), probs\n",
    "    #Does not performs the dislpacement of node here, only predicts the node and displacement vector                       \n",
    "          \n",
    "\n",
    "\n",
    "#Discounted reward function\n",
    "@jax.jit\n",
    "def get_discounted_returns(Rewards,Y=0.9):\n",
    "    \"\"\"Calculates discounted rewards\"\"\"\n",
    "    res=jnp.zeros(Rewards.shape)\n",
    "    #res=Rewards\n",
    "    Temp_G=onp.zeros((Rewards.shape[0],))\n",
    "    for k in range(Rewards.shape[1]-1,-1,-1):\n",
    "        Temp_G=Rewards[:,k]+Y*Temp_G\n",
    "        res=res.at[:,k].set(Temp_G)\n",
    "    return res\n",
    "    \n",
    "#Defining loss function\n",
    "@jax.jit\n",
    "def Traj_Loss_fn(*,log_probs, Returns):\n",
    "    return jnp.sum(log_probs*Returns,axis=1)\n",
    "\n",
    "#Grad_add function\n",
    "@jax.jit\n",
    "def add_grads(grad1,grad2):\n",
    "    return jax.tree_map(lambda x,y:x+y,grad1,grad2)\n",
    "\n",
    "@jax.jit\n",
    "def scalar_mult_grad(k,grad):\n",
    "    return jax.tree_map(lambda x:k*x,grad)\n",
    "\n",
    "def print_log(log,is_plot=False,epoch_id=0,Batch_id=0):\n",
    "    B_sz=log['Reward'].shape[0]\n",
    "    log_length=log['Reward'].shape[1]\n",
    "    if(is_plot==True):\n",
    "        for i in range(len(log['States'])):\n",
    "            Sys.plot_batched(log['States'][i],epoch_id=epoch_id,batch_id=Batch_id,step_id=i,node_ids=log['Node_id'][i],Edges=True,save=False)\n",
    "            #Sys.plot_frame_edge(ax1,log['States'][i],node_id=int(log['Node_id'][i]))\n",
    "    for k in range(B_sz):\n",
    "        print(\"\\n#GraphNo. \",k+1)\n",
    "        print(\"\\nStep\\tMax_Mu\\tMean_Mu\\t   Max_Disp  Mean_Disp\\tLog_Total_prob\\tReward\\t d_PE\\t  PE\")\n",
    "        for i in range(log['Reward'].shape[1]):\n",
    "            print(i+1,\"\\t%8.5f  %8.5f  %8.5f  %8.5f  %8.5f  %8.5f  %8.5f  %8.5f\"%(log['Max_Mu'][k][i],log['Mean_Mu'][k][i],log['Max_Disp'][k][i],log['Mean_Disp'][k][i],log['Total_prob'][k][i],log['Reward'][k][i],log['d_PE'][k][i],log['PE'][k][i]))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1029908.49464028 -1029926.15007278 -1029971.64720984 -1030044.89145933\n",
      " -1030145.60295065 -1030273.13941423 -1030426.29824724 -1030603.29639151\n",
      " -1030802.33444609 -1031022.34919555 -1031262.429675   -1031520.87204498\n",
      " -1031795.7888478  -1032085.82260453 -1032389.51272812 -1032705.06057534\n",
      " -1033030.81348671 -1033365.35368776 -1033707.35922477 -1034055.61278503\n",
      " -1034409.00898816 -1034766.5437388  -1035127.31610485 -1035490.52969655\n",
      " -1035855.47073072 -1036221.5168519  -1036588.12343782 -1036954.80990915\n",
      " -1037321.16625166 -1037686.83758151 -1038051.52244854 -1038414.95822403\n",
      " -1038776.92697089 -1039137.25065287 -1039495.77567141 -1039852.38009444\n",
      " -1040206.95903613 -1040559.43489854 -1040909.74571237 -1041257.84492182\n",
      " -1041603.70145962 -1041947.28895437 -1042288.59742456 -1042627.62292729\n",
      " -1042964.36851668 -1043298.84314007 -1043631.05845305 -1043961.03688839\n",
      " -1044288.79757686 -1044614.36462925 -1044937.76659542 -1045259.02932403\n",
      " -1045578.18001264 -1045895.25131587 -1046210.27660412 -1046523.28303642\n",
      " -1046834.3046997  -1047143.37352976 -1047450.52138482 -1047755.77991907\n",
      " -1048059.17825443 -1048360.75401382 -1048660.5310034  -1048958.53918175\n",
      " -1049254.81238817 -1049549.37718045 -1049842.26171134 -1050133.49570965\n",
      " -1050423.10156439 -1050711.10766946 -1050997.54174683 -1051282.42417269\n",
      " -1051565.78132945 -1051847.63671717 -1052128.01095394 -1052406.9284711\n",
      " -1052684.41307075 -1052960.48349211 -1053235.16010148 -1053508.46268236\n",
      " -1053780.4104474  -1054051.02205918 -1054320.31564485 -1054588.30881256\n",
      " -1054855.01866778 -1055120.46182955 -1055384.65444653 -1055647.60776505\n",
      " -1055909.34594032 -1056169.87935112 -1056429.2224194  -1056687.38694793\n",
      " -1056944.38880879 -1057200.24350604 -1057454.96174007 -1057708.55387081\n",
      " -1057961.03660531 -1058212.41965613 -1058462.7146568  -1058711.93294653]\n"
     ]
    }
   ],
   "source": [
    "#Defining model\n",
    "\n",
    "#from JMDSystem import MDTuple, My_system\n",
    "key = random.PRNGKey(119)\n",
    "Sys=Generic_system()\n",
    "\n",
    "Train,Val,Test, shift_fn,Batch_pair_cutoffs,Batch_pair_sigma,Batch_Disp_Vec_fn,Batch_Node_energy_fn,Batch_Total_energy_fn,displacement_fn, shift_fn,Disp_Vec_fn =Sys.create_batched_States(random.PRNGKey(147),System='CSH',spatial_dimension=3,N=152, N_sample =100,Batch_size=3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_R=[Systate_temp.R[k] for k in range(B_sz)]\n",
    "    \n",
    "Batch_Total_energy_fn(FIRE_desc(1e-2,len_ep,Test_R[k],Total_energy_fn,shift_fn)[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Total_energy_fn(R):\n",
    "    return Batch_Total_energy_fn(R[jnp.newaxis,:,:])[0]\n",
    "Total_energy_fn=jax.jit(Total_energy_fn)\n",
    "\n",
    "model=Pol_Net(edge_emb_size=48\n",
    "    ,node_emb_size=48\n",
    "    ,fa_layers=2\n",
    "    ,fb_layers=2\n",
    "    ,fv_layers=2\n",
    "    ,fe_layers=2\n",
    "    ,MLP1_layers=1\n",
    "    ,MLP2_layers=4\n",
    "    ,spatial_dim=3\n",
    "    ,sigma=50\n",
    "    ,train=True\n",
    "    ,message_passing_steps=1)\n",
    "    \n",
    "#Initializing model parameters\n",
    "key1, key2 =random.split(random.PRNGKey(147), 2)\n",
    "params = model.init(key2, Train[0].Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def loss_fn(params,Systate,key,spatial_dim=3,len_ep=10,Batch_id=1):\n",
    "    len_ep=len_ep\n",
    "    K_disp=100\n",
    "    Batch_id=1\n",
    "    log_length=len_ep\n",
    "    B_sz=Systate.N.shape[0]\n",
    "    Systate_temp=Systate\n",
    "    apply_fn=model.apply\n",
    "    #apply_fn=jax.jit(model.apply)\n",
    "    log_length=len_ep\n",
    "    log = {\n",
    "        'Max_Mu': jnp.zeros((B_sz,log_length,)),    \n",
    "        'Max_Disp': jnp.zeros((B_sz,log_length,)),\n",
    "        'Mean_Mu': jnp.zeros((B_sz,log_length,)),    \n",
    "        'Mean_Disp': jnp.zeros((B_sz,log_length,)),\n",
    "        #'Disp_prob':jnp.zeros((B_sz,log_length,)),\n",
    "        'Total_prob':jnp.zeros((B_sz,log_length,)),\n",
    "        'Reward':jnp.zeros((B_sz,log_length,)),\n",
    "        'd_PE':jnp.zeros((B_sz,log_length,)),\n",
    "        'PE':jnp.zeros((B_sz,log_length,)),\n",
    "        'States':[]}\n",
    "    #Systate_temp=Systate\n",
    "    Test_R=[Systate_temp.R[k] for k in range(B_sz)]\n",
    "    PE1=onp.zeros((len_ep,B_sz))\n",
    "    for k in range(B_sz):\n",
    "        PE1[:,k]=Batch_Total_energy_fn(FIRE_desc(1e-2,len_ep,Test_R[k],Total_energy_fn,shift_fn)[1])\n",
    "    for i in range(len_ep):\n",
    "        key, key1,key2 = random.split(key, 3)\n",
    "    \n",
    "        #1: Pass through Policy_net\n",
    "        #Batch_G, Batch_node_probs, Batch_Mux_Muy = apply_fn(params, Systate_temp.Graph)\n",
    "        (Batch_G, Batch_node_probs, Batch_Mux_Muy),mutated_vars = apply_fn(params, Systate_temp.Graph,mutable=['batch_stats'])\n",
    "        #print(\"Batch_G.nodes\",Batch_G.nodes)\n",
    "        Batch_Mux_Muy=jnp.clip(Batch_Mux_Muy,-10,10)\n",
    "        #2: Choose node and disp from prob distributions\n",
    "        Batch_chosen_node_indeces,Batch_chosen_node_prob=Batch_choose_topK_e_greedy(Batch_Mux_Muy,Batch_node_probs,key=key1,B_sz=B_sz,K=K_disp,epsilon=1.0)#eps= 1.0 during training\n",
    "        #To choose k nodes\n",
    "        \n",
    "        Node_mask=jnp.zeros(Batch_Mux_Muy.reshape(B_sz,-1,spatial_dim).shape)\n",
    "        Node_mask=Node_mask.at[jnp.array([[i for i in range(Batch_chosen_node_indeces.shape[0])]]* Batch_chosen_node_indeces.shape[1]).T, Batch_chosen_node_indeces,:].set(1.0)\n",
    "        Batch_Disp_vec, Batch_log_disp_prob,Batch_prob_disp= Batch_pred_disp_vec(Batch_Mux_Muy.reshape(B_sz,-1,spatial_dim),key2,B_sz=B_sz,std=1e-6)\n",
    "        #Batch_log_node_prob=jnp.log(Batch_chosen_node_prob)\n",
    "        #print(Batch_prob_disp)\n",
    "        #print(Batch_log_prob,Batch_log_node_prob)\n",
    "        #print(Batch_chosen_Mux_Muy-Batch_Disp_vec)\n",
    "        #print(jnp.sum(Batch_log_disp_prob),jnp.sum(Batch_log_node_prob))\n",
    "    \n",
    "        #print(jnp.sum(Batch_log_disp_prob+Batch_log_node_prob))\n",
    "        #print(jnp.sum(Batch_log_disp_prob*Node_mask[:,:,0],axis=1).shape)\n",
    "        Log_Pi_a_given_s=jnp.sum(Batch_log_disp_prob*Node_mask[:,:,0],axis=1)\n",
    "        #3: Displace all nodes with predicted displacement\n",
    "    #     Batch_Disp_vec_pred=Batch_Mux_Muy.at[Batch_chosen_node_indeces].set(Batch_Disp_vec)\n",
    "    #     Node_indeces=jnp.array([[i for i in range(Systate.R.shape[1])] for k in range(B_sz)])\n",
    "        Systate_new,Batch_d_PE=Sys.multi_disp_node(Batch_Disp_vec*Node_mask,Systate_temp,shift_fn,Batch_pair_cutoffs,Batch_pair_sigma,Batch_Disp_Vec_fn,Batch_Node_energy_fn,Batch_Total_energy_fn)\n",
    "        log['d_PE']=log['d_PE'].at[:,i].set(Batch_d_PE)\n",
    "        log['PE']=log['PE'].at[:,i].set(Systate_temp.pe)\n",
    "        #log['Node_id']=log['Node_id'].at[:,i].set(Batch_chosen_node_index)\n",
    "        #log['Node_prob']=log['Node_prob'].at[:,i].set(Batch_chosen_node_prob.reshape(-1))\n",
    "        Mu_magnitude=jnp.sum(Batch_Mux_Muy**2,axis=1).reshape((B_sz,-1,))\n",
    "        Disp_magnitude=jnp.sum(Batch_Disp_vec**2,axis=2)\n",
    "        \n",
    "        log['Max_Mu']=log['Max_Mu'].at[:,i].set(jnp.sqrt(jnp.max(Mu_magnitude,axis=1)))\n",
    "        log['Mean_Mu']=log['Mean_Mu'].at[:,i].set(jnp.sqrt(jnp.mean(Mu_magnitude,axis=1)))\n",
    "        \n",
    "        log['Max_Disp']=log['Max_Disp'].at[:,i].set(jnp.sqrt(jnp.max(Disp_magnitude,axis=1)))\n",
    "        log['Mean_Disp']=log['Mean_Disp'].at[:,i].set(jnp.sqrt(jnp.mean(Disp_magnitude,axis=1)))\n",
    "        \n",
    "        \n",
    "        #log['Disp_prob']=log['Disp_prob'].at[:,i].set(jnp.exp(jnp.sum(Batch_log_prob)))\n",
    "        log['Total_prob']=log['Total_prob'].at[:,i].set(Log_Pi_a_given_s/10)\n",
    "        log['Reward']=log['Reward'].at[:,i].set(-1*(Systate_new.pe-PE1[-1,:])+jnp.exp(-(0.05)*(Systate_new.pe-PE1[-1,:])))\n",
    "        log['States']+=[Systate_temp]\n",
    "        #Update current state\n",
    "        Systate_temp=Systate_new\n",
    "    loss_batch=Traj_Loss_fn(log_probs=log['Total_prob'],Returns=get_discounted_returns(log['Reward']))  #Shape: (B_sz,)\n",
    "    #Taking sum of loss\n",
    "    loss=jnp.sum(loss_batch)/B_sz\n",
    "    return loss, (Systate_temp,log)    #Returns updated graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 10:48:15.020032: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/civil/btech/ce1180169/anaconda3/envs/JaxEqv_Graph/lib\n",
      "2023-01-16 10:48:15.020637: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/civil/btech/ce1180169/anaconda3/envs/JaxEqv_Graph/lib\n",
      "2023-01-16 10:48:15.020664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:absl:GlobalAsyncCheckpointManager is not imported correctly. Checkpointing of GlobalDeviceArrays will not be available.To use the feature, install tensorstore.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#GraphNo.  1\n",
      "\n",
      "Step\tMax_Mu\tMean_Mu\t   Max_Disp  Mean_Disp\tLog_Total_prob\tReward\t d_PE\t  PE\n",
      "1 \t 0.07082   0.06177   0.07082   0.06177  -15.00247  -17338.77741  -122.73729  -1051282.42417\n",
      "2 \t 0.07082   0.06173   0.07082   0.06173  -15.72501  -17226.46423  -112.31317  -1051405.16146\n",
      "3 \t 0.07082   0.06165   0.07082   0.06165  -14.42475  -17285.71634  59.25211  -1051517.47463\n",
      "4 \t 0.07082   0.06164   0.07082   0.06164  -16.34712  -17444.24633  158.52999  -1051458.22253\n",
      "5 \t 0.07082   0.06152   0.07082   0.06152  -13.82348  -17774.33139  330.08506  -1051299.69253\n",
      "\n",
      "#GraphNo.  2\n",
      "\n",
      "Step\tMax_Mu\tMean_Mu\t   Max_Disp  Mean_Disp\tLog_Total_prob\tReward\t d_PE\t  PE\n",
      "1 \t 0.07082   0.06195   0.07082   0.06195  -13.82765  -25327.09366  -132.84095  -1032085.82260\n",
      "2 \t 0.07082   0.06189   0.07082   0.06189  -12.54239  -25348.47027  21.37660  -1032218.66356\n",
      "3 \t 0.07082   0.06191   0.07082   0.06191  -14.63570  -25368.07904  19.60878  -1032197.28695\n",
      "4 \t 0.07082   0.06181   0.07082   0.06181  -14.75977  -25480.31708  112.23804  -1032177.67818\n",
      "5 \t 0.07082   0.06153   0.07082   0.06153  -14.11566  -25675.58010  195.26302  -1032065.44014\n",
      "\n",
      "#GraphNo.  3\n",
      "\n",
      "Step\tMax_Mu\tMean_Mu\t   Max_Disp  Mean_Disp\tLog_Total_prob\tReward\t d_PE\t  PE\n",
      "1 \t 0.07082   0.06141   0.07081   0.06141  -15.48872  -15741.64948  -204.55946  -1055909.34594\n",
      "2 \t 0.07082   0.06148   0.07082   0.06148  -16.31732  -15662.41596  -79.23352  -1056113.90540\n",
      "3 \t 0.07082   0.06158   0.07082   0.06158  -13.78384  -15731.95505  69.53908  -1056193.13892\n",
      "4 \t 0.07082   0.06150   0.07082   0.06150  -16.21914  -15798.41848  66.46343  -1056123.59983\n",
      "5 \t 0.07082   0.06148   0.07082   0.06148  -15.59517  -15975.35115  176.93268  -1056057.13640\n",
      "Initial Loss 3776559.6444234005\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "#Defining optimizer\n",
    "import optax\n",
    "import flax\n",
    "from flax.training import train_state\n",
    "from flax import serialization\n",
    "from flax.training import checkpoints as ckp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#config.update('jax_disable_jit', True)\n",
    "#config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "#schedule = optax.warmup_cosine_decay_schedule(\n",
    "#  init_value=1e-8,\n",
    "#  peak_value=0.001,\n",
    "#   warmup_steps=50,\n",
    "#   decay_steps=500,\n",
    "#   end_value=0.0,\n",
    "# )\n",
    "\n",
    "tx = optax.chain(\n",
    "  optax.clip(0.5),\n",
    "  optax.adam(learning_rate=0.005)\n",
    ")\n",
    "#tx=flax.optim.momentum(learning_rate=1e-3,beta=0.9,weight_decay=1,nestrov=True)\n",
    "\n",
    "#tx = optax.sgd(learning_rate=0.001)\n",
    "#Add l2 regularizer\n",
    "#print(params)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "#Initial loss and gradients computation\n",
    "((loss,(Systate_init,log_init)),init_grad)=loss_grad_fn(params,Train[0],random.PRNGKey(147))\n",
    "print_log(log_init)\n",
    "print(\"Initial Loss\",loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#GraphNo.  1\n",
      "\n",
      "Step\tMax_Mu\tMean_Mu\t   Max_Disp  Mean_Disp\tLog_Total_prob\tReward\t d_PE\t  PE\n",
      "1 \t 0.07082   0.06177   0.07082   0.06177  -15.00247  -17338.77741  -122.73729  -1051282.42417\n",
      "2 \t 0.07082   0.06173   0.07082   0.06173  -15.72501  -17226.46423  -112.31317  -1051405.16146\n",
      "3 \t 0.07082   0.06165   0.07082   0.06165  -14.42475  -17285.71634  59.25211  -1051517.47463\n",
      "4 \t 0.07082   0.06164   0.07082   0.06164  -16.34712  -17444.24633  158.52999  -1051458.22253\n",
      "5 \t 0.07082   0.06152   0.07082   0.06152  -13.82348  -17774.33139  330.08506  -1051299.69253\n",
      "\n",
      "#GraphNo.  2\n",
      "\n",
      "Step\tMax_Mu\tMean_Mu\t   Max_Disp  Mean_Disp\tLog_Total_prob\tReward\t d_PE\t  PE\n",
      "1 \t 0.07082   0.06195   0.07082   0.06195  -13.82765  -25327.09366  -132.84095  -1032085.82260\n",
      "2 \t 0.07082   0.06189   0.07082   0.06189  -12.54239  -25348.47027  21.37660  -1032218.66356\n",
      "3 \t 0.07082   0.06191   0.07082   0.06191  -14.63570  -25368.07904  19.60878  -1032197.28695\n",
      "4 \t 0.07082   0.06181   0.07082   0.06181  -14.75977  -25480.31708  112.23804  -1032177.67818\n",
      "5 \t 0.07082   0.06153   0.07082   0.06153  -14.11566  -25675.58010  195.26302  -1032065.44014\n",
      "\n",
      "#GraphNo.  3\n",
      "\n",
      "Step\tMax_Mu\tMean_Mu\t   Max_Disp  Mean_Disp\tLog_Total_prob\tReward\t d_PE\t  PE\n",
      "1 \t 0.07082   0.06141   0.07081   0.06141  -15.48872  -15741.64948  -204.55946  -1055909.34594\n",
      "2 \t 0.07082   0.06148   0.07082   0.06148  -16.31732  -15662.41596  -79.23352  -1056113.90540\n",
      "3 \t 0.07082   0.06158   0.07082   0.06158  -13.78384  -15731.95505  69.53908  -1056193.13892\n",
      "4 \t 0.07082   0.06150   0.07082   0.06150  -16.21914  -15798.41848  66.46343  -1056123.59983\n",
      "5 \t 0.07082   0.06148   0.07082   0.06148  -15.59517  -15975.35115  176.93268  -1056057.13640\n",
      "Initial Loss 3776559.6444234005\n",
      "Batch:  0   100633737.43474638\n",
      "\n",
      "#GraphNo.  1\n",
      "\n",
      "Step\tMax_Mu\tMean_Mu\t   Max_Disp  Mean_Disp\tLog_Total_prob\tReward\t d_PE\t  PE\n",
      "1 \t 0.07082   0.06177   0.07082   0.06177  -14.71140  -78624.30595  -240.78878  -1051282.42417\n",
      "2 \t 0.07082   0.06175   0.07082   0.06175  -17.36715  -78541.33139  -82.97456  -1051523.21296\n",
      "3 \t 0.07082   0.06168   0.07082   0.06168  -13.74416  -78625.49407  84.16268  -1051606.18752\n",
      "4 \t 0.07082   0.06165   0.07082   0.06165  -15.99154  -78700.93721  75.44313  -1051522.02484\n",
      "5 \t 0.07082   0.06149   0.07082   0.06149  -16.27387  -78948.89171  247.95450  -1051446.58171\n",
      "6 \t 0.07082   0.06153   0.07082   0.06153  -17.51089  -79176.09627  227.20456  -1051198.62721\n",
      "7 \t 0.07082   0.06141   0.07082   0.06141  -14.39239  -79556.77565  380.67939  -1050971.42264\n",
      "8 \t 0.07082   0.06148   0.07082   0.06148  -13.87475  -80131.60358  574.82792  -1050590.74326\n",
      "9 \t 0.07082   0.06136   0.07082   0.06136  -14.98954  -80720.73180  589.12822  -1050015.91534\n",
      "10 \t 0.07082   0.06130   0.07082   0.06130  -16.81391  -81215.13930  494.40750  -1049426.78711\n",
      "11 \t 0.07082   0.06097   0.07082   0.06097  -12.69949  -81934.88950  719.75020  -1048932.37961\n",
      "12 \t 0.07082   0.06122   0.07082   0.06122  -15.29356  -82879.58191  944.69240  -1048212.62941\n",
      "13 \t 0.07082   0.06085   0.07082   0.06085  -15.93041  -83723.21727  843.63536  -1047267.93701\n",
      "14 \t 0.07082   0.06109   0.07082   0.06109  -15.15661  -84627.72814  904.51087  -1046424.30164\n",
      "15 \t 0.07082   0.06073   0.07082   0.06073  -17.21177  -85723.37465  1095.64651  -1045519.79077\n",
      "\n",
      "#GraphNo.  2\n",
      "\n",
      "Step\tMax_Mu\tMean_Mu\t   Max_Disp  Mean_Disp\tLog_Total_prob\tReward\t d_PE\t  PE\n",
      "1 \t 0.07082   0.06195   0.07082   0.06195  -14.35598  -91445.21093  -256.83066  -1032085.82260\n",
      "2 \t 0.07082   0.06189   0.07082   0.06189  -14.75468  -91360.52314  -84.68779  -1032342.65326\n",
      "3 \t 0.07082   0.06189   0.07082   0.06189  -13.80848  -91330.18935  -30.33379  -1032427.34105\n",
      "4 \t 0.07082   0.06170   0.07082   0.06170  -15.29023  -91424.03094  93.84159  -1032457.67484\n",
      "5 \t 0.07082   0.06181   0.07082   0.06181  -14.61799  -91618.88486  194.85392  -1032363.83325\n",
      "6 \t 0.07082   0.06149   0.07082   0.06149  -17.02645  -91834.07294  215.18808  -1032168.97933\n",
      "7 \t 0.07082   0.06155   0.07082   0.06155  -14.79571  -92239.00455  404.93161  -1031953.79125\n",
      "8 \t 0.07082   0.06137   0.07082   0.06137  -13.78560  -92624.01629  385.01174  -1031548.85964\n",
      "9 \t 0.07082   0.06143   0.07082   0.06143  -14.53206  -93254.41937  630.40308  -1031163.84790\n",
      "10 \t 0.07082   0.06156   0.07082   0.06156  -16.80081  -93877.40427  622.98490  -1030533.44482\n",
      "11 \t 0.07082   0.06131   0.07082   0.06131  -16.78186  -94777.24262  899.83835  -1029910.45992\n",
      "12 \t 0.07082   0.06123   0.07082   0.06123  -12.73973  -95507.65629  730.41367  -1029010.62158\n",
      "13 \t 0.07082   0.06146   0.07082   0.06146  -13.96178  -96561.48435  1053.82807  -1028280.20790\n",
      "14 \t 0.07082   0.06126   0.07082   0.06126  -14.23044  -97607.17435  1045.69000  -1027226.37984\n",
      "15 \t 0.07082   0.06137   0.07082   0.06137  -15.30524  -98917.81810  1310.64375  -1026180.68984\n",
      "\n",
      "#GraphNo.  3\n",
      "\n",
      "Step\tMax_Mu\tMean_Mu\t   Max_Disp  Mean_Disp\tLog_Total_prob\tReward\t d_PE\t  PE\n",
      "1 \t 0.07082   0.06141   0.07082   0.06141  -13.88575  -75917.03520  -150.68164  -1055909.34594\n",
      "2 \t 0.07082   0.06139   0.07082   0.06139  -15.07974  -75795.25022  -121.78499  -1056060.02758\n",
      "3 \t 0.07082   0.06148   0.07082   0.06148  -13.76634  -75812.34306  17.09285  -1056181.81257\n",
      "4 \t 0.07082   0.06151   0.07082   0.06151  -12.94515  -75942.25481  129.91174  -1056164.71972\n",
      "5 \t 0.07082   0.06155   0.07082   0.06155  -16.37824  -76128.67936  186.42455  -1056034.80798\n",
      "6 \t 0.07082   0.06148   0.07081   0.06148  -17.54629  -76457.95616  329.27680  -1055848.38342\n",
      "7 \t 0.07082   0.06150   0.07082   0.06150  -14.42148  -76777.23082  319.27466  -1055519.10662\n",
      "8 \t 0.07082   0.06136   0.07082   0.06136  -15.23635  -77266.75905  489.52823  -1055199.83196\n",
      "9 \t 0.07082   0.06134   0.07082   0.06134  -14.64578  -77701.87946  435.12041  -1054710.30373\n",
      "10 \t 0.07082   0.06129   0.07082   0.06129  -17.35314  -78243.54047  541.66101  -1054275.18332\n",
      "11 \t 0.07082   0.06122   0.07082   0.06122  -14.06922  -78860.58960  617.04913  -1053733.52232\n",
      "12 \t 0.07082   0.06116   0.07082   0.06116  -14.11778  -79816.78053  956.19093  -1053116.47319\n",
      "13 \t 0.07082   0.06105   0.07082   0.06105  -15.04373  -80772.36874  955.58822  -1052160.28226\n",
      "14 \t 0.07082   0.06070   0.07082   0.06070  -15.95785  -81776.02004  1003.65130  -1051204.69404\n",
      "15 \t 0.07082   0.06062   0.07082   0.06062  -15.69302  -82847.90620  1071.88616  -1050201.04275\n",
      "Batch:  1   99031033.93243599\n",
      "\n",
      "#GraphNo.  1\n",
      "\n",
      "Step\tMax_Mu\tMean_Mu\t   Max_Disp  Mean_Disp\tLog_Total_prob\tReward\t d_PE\t  PE\n",
      "1 \t 0.07082   0.06141   0.07082   0.06141  -13.83840  -76052.19159  -170.77918  -1055647.60777\n",
      "2 \t 0.07082   0.06149   0.07082   0.06149  -18.16751  -76037.64342  -14.54817  -1055818.38694\n",
      "3 \t 0.07082   0.06157   0.07082   0.06157  -15.03603  -76072.15209  34.50867  -1055832.93511\n",
      "4 \t 0.07082   0.06151   0.07082   0.06151  -13.34446  -76138.91855  66.76647  -1055798.42644\n",
      "5 \t 0.07082   0.06147   0.07082   0.06147  -15.06746  -76287.07209  148.15354  -1055731.65998\n",
      "6 \t 0.07082   0.06145   0.07082   0.06145  -14.28014  -76563.76541  276.69332  -1055583.50644\n",
      "7 \t 0.07082   0.06134   0.07082   0.06134  -14.33590  -76953.50133  389.73592  -1055306.81312\n",
      "8 \t 0.07082   0.06131   0.07082   0.06131  -16.22667  -77457.59083  504.08951  -1054917.07720\n",
      "9 \t 0.07082   0.06127   0.07082   0.06127  -13.59927  -77975.99649  518.40566  -1054412.98770\n",
      "10 \t 0.07082   0.06126   0.07082   0.06126  -16.48168  -78813.36960  837.37311  -1053894.58204\n",
      "11 \t 0.07082   0.06081   0.07082   0.06081  -13.78128  -79539.92312  726.55352  -1053057.20893\n",
      "12 \t 0.07082   0.06080   0.07082   0.06080  -15.43172  -80510.65291  970.72978  -1052330.65541\n",
      "13 \t 0.07082   0.06069   0.07082   0.06069  -14.63064  -81205.56741  694.91450  -1051359.92562\n",
      "14 \t 0.07082   0.06103   0.07082   0.06103  -13.91338  -82417.92754  1212.36013  -1050665.01112\n",
      "15 \t 0.07082   0.06059   0.07082   0.06059  -17.07823  -83470.09107  1052.16353  -1049452.65099\n",
      "\n",
      "#GraphNo.  2\n",
      "\n",
      "Step\tMax_Mu\tMean_Mu\t   Max_Disp  Mean_Disp\tLog_Total_prob\tReward\t d_PE\t  PE\n",
      "1 \t 0.07082   0.06207   0.07082   0.06207  -15.12843  -90406.41216  -149.80804  -1033707.35922\n",
      "2 \t 0.07082   0.06187   0.07082   0.06187  -14.06253  -90391.16580  -15.24635  -1033857.16726\n",
      "3 \t 0.07082   0.06184   0.07082   0.06184  -16.15458  -90374.04800  -17.11780  -1033872.41362\n",
      "4 \t 0.07082   0.06166   0.07082   0.06166  -14.86849  -90486.04462  111.99662  -1033889.53141\n",
      "5 \t 0.07082   0.06159   0.07082   0.06159  -13.85651  -90627.68748  141.64286  -1033777.53480\n",
      "6 \t 0.07082   0.06161   0.07082   0.06161  -14.15653  -90930.34261  302.65513  -1033635.89194\n",
      "7 \t 0.07082   0.06154   0.07082   0.06154  -13.87069  -91400.35495  470.01234  -1033333.23681\n",
      "8 \t 0.07082   0.06141   0.07082   0.06141  -14.85410  -91943.55661  543.20167  -1032863.22447\n",
      "9 \t 0.07082   0.06123   0.07082   0.06123  -15.33802  -92538.97004  595.41343  -1032320.02281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t 0.07082   0.06150   0.07082   0.06150  -14.86562  -93298.96259  759.99255  -1031724.60938\n",
      "11 \t 0.07082   0.06110   0.07082   0.06110  -12.47190  -94118.86392  819.90133  -1030964.61683\n",
      "12 \t 0.07082   0.06162   0.07082   0.06162  -13.70016  -94943.49003  824.62611  -1030144.71550\n",
      "13 \t 0.07082   0.06136   0.07082   0.06136  -17.50707  -95953.52270  1010.03266  -1029320.08938\n",
      "14 \t 0.07082   0.06144   0.07082   0.06144  -14.63734  -97151.35415  1197.83145  -1028310.05672\n",
      "15 \t 0.07082   0.06127   0.07082   0.06127  -15.15121  -98168.64706  1017.29291  -1027112.22527\n",
      "\n",
      "#GraphNo.  3\n",
      "\n",
      "Step\tMax_Mu\tMean_Mu\t   Max_Disp  Mean_Disp\tLog_Total_prob\tReward\t d_PE\t  PE\n",
      "1 \t 0.07082   0.06140   0.07082   0.06140  -16.39374  -75634.16193  -126.20011  -1056429.22242\n",
      "2 \t 0.07082   0.06139   0.07082   0.06139  -14.39656  -75479.07686  -155.08507  -1056555.42253\n",
      "3 \t 0.07082   0.06147   0.07082   0.06147  -15.03966  -75482.62904   3.55218  -1056710.50759\n",
      "4 \t 0.07082   0.06157   0.07082   0.06157  -13.18098  -75490.98804   8.35900  -1056706.95541\n",
      "5 \t 0.07082   0.06142   0.07082   0.06142  -18.09515  -75619.52401  128.53597  -1056698.59641\n",
      "6 \t 0.07082   0.06138   0.07082   0.06138  -15.48701  -75896.20844  276.68443  -1056570.06044\n",
      "7 \t 0.07082   0.06144   0.07082   0.06144  -17.24117  -76156.04406  259.83561  -1056293.37601\n",
      "8 \t 0.07082   0.06139   0.07082   0.06139  -17.04728  -76593.82746  437.78341  -1056033.54040\n",
      "9 \t 0.07082   0.06127   0.07082   0.06127  -13.38408  -77133.42696  539.59950  -1055595.75699\n",
      "10 \t 0.07082   0.06126   0.07082   0.06126  -15.42668  -77906.56017  773.13321  -1055056.15749\n",
      "11 \t 0.07082   0.06111   0.07082   0.06111  -15.84940  -78722.36071  815.80053  -1054283.02428\n",
      "12 \t 0.07082   0.06081   0.07082   0.06081  -15.71489  -79499.36384  777.00314  -1053467.22375\n",
      "13 \t 0.07082   0.06088   0.07082   0.06088  -15.77365  -80589.62028  1090.25644  -1052690.22061\n",
      "14 \t 0.07082   0.06109   0.07082   0.06109  -15.04770  -81907.33439  1317.71411  -1051599.96417\n",
      "15 \t 0.07082   0.06078   0.07082   0.06078  -15.98444  -83070.99221  1163.65782  -1050282.25006\n",
      "Loss step 0:99832385.68359119 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m Batch_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Batch_size):\n\u001b[0;32m---> 68\u001b[0m     ((loss_val,(Systate,log)),grads) \u001b[38;5;241m=\u001b[39m \u001b[43mloss_grad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBatch_size\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkeys2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlen_ep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlen_ep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch: \u001b[39m\u001b[38;5;124m\"\u001b[39m,p,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,loss_val)\n\u001b[1;32m     70\u001b[0m     print_log(log,is_plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,epoch_id\u001b[38;5;241m=\u001b[39mi,Batch_id\u001b[38;5;241m=\u001b[39mp)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/_src/api.py:1167\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.value_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m   ans, vjp_py \u001b[38;5;241m=\u001b[39m _vjp(f_partial, \u001b[38;5;241m*\u001b[39mdyn_args, reduce_axes\u001b[38;5;241m=\u001b[39mreduce_axes)\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m   ans, vjp_py, aux \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m      \u001b[49m\u001b[43mf_partial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdyn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_axes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m _check_scalar(ans)\n\u001b[1;32m   1170\u001b[0m tree_map(partial(_check_output_dtype_grad, holomorphic), ans)\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/_src/api.py:2659\u001b[0m, in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, reduce_axes, *primals)\u001b[0m\n\u001b[1;32m   2657\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2658\u001b[0m   flat_fun, out_aux_trees \u001b[38;5;241m=\u001b[39m flatten_fun_nokwargs2(fun, in_tree)\n\u001b[0;32m-> 2659\u001b[0m   out_primal, out_vjp, aux \u001b[38;5;241m=\u001b[39m \u001b[43mad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvjp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2660\u001b[0m \u001b[43m      \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_axes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2661\u001b[0m   out_tree, aux_tree \u001b[38;5;241m=\u001b[39m out_aux_trees()\n\u001b[1;32m   2662\u001b[0m out_primal_py \u001b[38;5;241m=\u001b[39m tree_unflatten(out_tree, out_primal)\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/interpreters/ad.py:137\u001b[0m, in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux, reduce_axes)\u001b[0m\n\u001b[1;32m    135\u001b[0m   out_primals, pvals, jaxpr, consts \u001b[38;5;241m=\u001b[39m linearize(traceable, \u001b[38;5;241m*\u001b[39mprimals)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m   out_primals, pvals, jaxpr, consts, aux \u001b[38;5;241m=\u001b[39m \u001b[43mlinearize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraceable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munbound_vjp\u001b[39m(pvals, jaxpr, consts, \u001b[38;5;241m*\u001b[39mcts):\n\u001b[1;32m    140\u001b[0m   cts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ct \u001b[38;5;28;01mfor\u001b[39;00m ct, pval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cts, pvals) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pval\u001b[38;5;241m.\u001b[39mis_known())\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/interpreters/ad.py:124\u001b[0m, in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m _, in_tree \u001b[38;5;241m=\u001b[39m tree_flatten(((primals, primals), {}))\n\u001b[1;32m    123\u001b[0m jvpfun_flat, out_tree \u001b[38;5;241m=\u001b[39m flatten_fun(jvpfun, in_tree)\n\u001b[0;32m--> 124\u001b[0m jaxpr, out_pvals, consts \u001b[38;5;241m=\u001b[39m \u001b[43mpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_to_jaxpr_nounits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvpfun_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_pvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m out_primals_pvals, out_tangents_pvals \u001b[38;5;241m=\u001b[39m tree_unflatten(out_tree(), out_pvals)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(out_primal_pval\u001b[38;5;241m.\u001b[39mis_known() \u001b[38;5;28;01mfor\u001b[39;00m out_primal_pval \u001b[38;5;129;01min\u001b[39;00m out_primals_pvals)\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/interpreters/partial_eval.py:767\u001b[0m, in \u001b[0;36mtrace_to_jaxpr_nounits\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m core\u001b[38;5;241m.\u001b[39mnew_main(JaxprTrace, name_stack\u001b[38;5;241m=\u001b[39mcurrent_name_stack) \u001b[38;5;28;01mas\u001b[39;00m main:\n\u001b[1;32m    766\u001b[0m   fun \u001b[38;5;241m=\u001b[39m trace_to_subjaxpr_nounits(fun, main, instantiate)\n\u001b[0;32m--> 767\u001b[0m   jaxpr, (out_pvals, consts, env) \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env\n\u001b[1;32m    769\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m main, fun, env\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/linear_util.py:167\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    170\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    172\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    173\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, Systate, key, spatial_dim, len_ep, Batch_id)\u001b[0m\n\u001b[1;32m     29\u001b[0m key, key1,key2 \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(key, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#1: Pass through Policy_net\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#Batch_G, Batch_node_probs, Batch_Mux_Muy = apply_fn(params, Systate_temp.Graph)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m (Batch_G, Batch_node_probs, Batch_Mux_Muy),mutated_vars \u001b[38;5;241m=\u001b[39m \u001b[43mapply_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSystate_temp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmutable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_stats\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#print(\"Batch_G.nodes\",Batch_G.nodes)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m Batch_Mux_Muy\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mclip(Batch_Mux_Muy,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.05\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/flax/linen/module.py:1243\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, variables, rngs, method, mutable, capture_intermediates, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m   method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m\n\u001b[1;32m   1242\u001b[0m method \u001b[38;5;241m=\u001b[39m _get_unbound_fn(method)\n\u001b[0;32m-> 1243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmutable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmutable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_intermediates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrngs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrngs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/flax/core/scope.py:865\u001b[0m, in \u001b[0;36mapply.<locals>.wrapper\u001b[0;34m(variables, rngs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mApplyScopeInvalidVariablesStructureError(variables)\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m bind(variables, rngs\u001b[38;5;241m=\u001b[39mrngs, mutable\u001b[38;5;241m=\u001b[39mmutable,\n\u001b[1;32m    864\u001b[0m           flags\u001b[38;5;241m=\u001b[39mflags)\u001b[38;5;241m.\u001b[39mtemporary() \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[0;32m--> 865\u001b[0m   y \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mutable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m y, root\u001b[38;5;241m.\u001b[39mmutable_variables()\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/flax/linen/module.py:1689\u001b[0m, in \u001b[0;36mapply.<locals>.scope_fn\u001b[0;34m(scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1687\u001b[0m _context\u001b[38;5;241m.\u001b[39mcapture_stack\u001b[38;5;241m.\u001b[39mappend(capture_intermediates)\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1689\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   _context\u001b[38;5;241m.\u001b[39mcapture_stack\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/flax/linen/module.py:402\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    401\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 402\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/flax/linen/module.py:705\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m    704\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m--> 705\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m   y \u001b[38;5;241m=\u001b[39m fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch/civil/btech/ce1180169/Phase3_Training_CSH/Baseline_CSH1/models.py:484\u001b[0m, in \u001b[0;36mPol_Net.__call__\u001b[0;34m(self, graphs)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r_vec \n\u001b[1;32m    483\u001b[0m G,node_probs\u001b[38;5;241m=\u001b[39mmyVijNet(graphs)                             \n\u001b[0;32m--> 484\u001b[0m Mux_Muy\u001b[38;5;241m=\u001b[39m\u001b[43mDisplace_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G, node_probs, Mux_Muy\n",
      "File \u001b[0;32m/scratch/civil/btech/ce1180169/Phase3_Training_CSH/Baseline_CSH1/models.py:475\u001b[0m, in \u001b[0;36mPol_Net.__call__.<locals>.Displace_node\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m    473\u001b[0m x\u001b[38;5;241m=\u001b[39mf_vec\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, lyr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMLP2_mlp):\n\u001b[0;32m--> 475\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlyr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMLP2_activation(x)\n\u001b[1;32m    479\u001b[0m r_vec\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma\u001b[38;5;241m*\u001b[39mx \n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/flax/linen/module.py:402\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    401\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 402\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/flax/linen/module.py:705\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m    704\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m--> 705\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m   y \u001b[38;5;241m=\u001b[39m fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/flax/linen/linear.py:195\u001b[0m, in \u001b[0;36mDense.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m   bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m inputs, kernel, bias \u001b[38;5;241m=\u001b[39m \u001b[43mpromote_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m y \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mdot_general(inputs, kernel,\n\u001b[1;32m    197\u001b[0m                     (((inputs\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,), (\u001b[38;5;241m0\u001b[39m,)), ((), ())),\n\u001b[1;32m    198\u001b[0m                     precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/flax/linen/dtypes.py:97\u001b[0m, in \u001b[0;36mpromote_dtype\u001b[0;34m(dtype, inexact, *args)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\"Promotes input arguments to a specified or inferred dtype.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mAll args are cast to the same dtype. See ``canonicalize_dtype`` for how\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m  The arguments cast to arrays of the same dtype.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m dtype \u001b[38;5;241m=\u001b[39m canonicalize_dtype(\u001b[38;5;241m*\u001b[39margs, dtype\u001b[38;5;241m=\u001b[39mdtype, inexact\u001b[38;5;241m=\u001b[39minexact)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [jnp\u001b[38;5;241m.\u001b[39masarray(x, dtype) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/flax/linen/dtypes.py:97\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\"Promotes input arguments to a specified or inferred dtype.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mAll args are cast to the same dtype. See ``canonicalize_dtype`` for how\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m  The arguments cast to arrays of the same dtype.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m dtype \u001b[38;5;241m=\u001b[39m canonicalize_dtype(\u001b[38;5;241m*\u001b[39margs, dtype\u001b[38;5;241m=\u001b[39mdtype, inexact\u001b[38;5;241m=\u001b[39minexact)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:2036\u001b[0m, in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m   2034\u001b[0m lax_internal\u001b[38;5;241m.\u001b[39m_check_user_dtype_supported(dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2035\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype) \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[0;32m-> 2036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:2000\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   1998\u001b[0m   out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mobject\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, ndmin\u001b[38;5;241m=\u001b[39mndmin, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1999\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, ndarray_types):\n\u001b[0;32m-> 2000\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2001\u001b[0m   out \u001b[38;5;241m=\u001b[39m _array_copy(\u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/interpreters/ad.py:456\u001b[0m, in \u001b[0;36mJVPTracer.aval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maval\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    455\u001b[0m   \u001b[38;5;66;03m# TODO(dougalm): add epsilon ball\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_aval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprimal\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/core.py:1253\u001b[0m, in \u001b[0;36mget_aval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1251\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39maval\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1253\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_aval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/core.py:1242\u001b[0m, in \u001b[0;36mconcrete_aval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m typ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__mro__\u001b[39m:\n\u001b[1;32m   1241\u001b[0m   handler \u001b[38;5;241m=\u001b[39m pytype_aval_mappings\u001b[38;5;241m.\u001b[39mget(typ)\n\u001b[0;32m-> 1242\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m handler: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__jax_array__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1244\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete_aval(x\u001b[38;5;241m.\u001b[39m__jax_array__())\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/_src/abstract_arrays.py:51\u001b[0m, in \u001b[0;36mcanonical_concrete_aval\u001b[0;34m(val, weak_type)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcanonical_concrete_aval\u001b[39m(val, weak_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mConcreteArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonicalize_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweak_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/JaxEqv_Graph/lib/python3.10/site-packages/jax/core.py:1451\u001b[0m, in \u001b[0;36mConcreteArray.__init__\u001b[0;34m(self, dtype, val, weak_type)\u001b[0m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype, val, weak_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1451\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m      \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_weakly_typed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1454\u001b[0m   \u001b[38;5;66;03m# Note: canonicalized self.dtype doesn't necessarily match self.val\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(np\u001b[38;5;241m.\u001b[39mresult_type(val)), (val, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Defining optimizer\n",
    "import optax\n",
    "import flax\n",
    "from flax.training import train_state\n",
    "from flax import serialization\n",
    "from flax.training import checkpoints as ckp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#config.update('jax_disable_jit', True)\n",
    "#config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "#schedule = optax.warmup_cosine_decay_schedule(\n",
    "#  init_value=1e-8,\n",
    "#  peak_value=0.001,\n",
    "#   warmup_steps=50,\n",
    "#   decay_steps=500,\n",
    "#   end_value=0.0,\n",
    "# )\n",
    "\n",
    "tx = optax.chain(\n",
    "  optax.clip(0.5),\n",
    "  optax.adam(learning_rate=0.005)\n",
    ")\n",
    "#tx=flax.optim.momentum(learning_rate=1e-3,beta=0.9,weight_decay=1,nestrov=True)\n",
    "\n",
    "#tx = optax.sgd(learning_rate=0.001)\n",
    "#Add l2 regularizer\n",
    "#print(params)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "#Initial loss and gradients computation\n",
    "((loss,(Systate_init,log_init)),init_grad)=loss_grad_fn(params,Train[0],random.PRNGKey(147))\n",
    "print_log(log_init)\n",
    "print(\"Initial Loss\",loss)\n",
    "\n",
    "\n",
    "\n",
    "#Training and Validation\n",
    "\n",
    "Train_loss_data   =  []\n",
    "Train_loss_epochs =  []\n",
    "Val_loss_data     =  []\n",
    "Val_loss_epochs   =  []\n",
    "Batch_size        =  2\n",
    "len_ep            =  15\n",
    "Max_dataset_size  =  300\n",
    "Print_freq        =  1\n",
    "plot_freq         =  100\n",
    "Model_save_freq   =  10\n",
    "N_epoch           =  800\n",
    "Val_freq          =  20\n",
    "Reset_dataset_every= 100\n",
    "keys1=random.split(random.PRNGKey(117721817),N_epoch)\n",
    "for i in range(N_epoch):\n",
    "    if((i+1)%Reset_dataset_every==0):\n",
    "        print(\"Re creating dataset with new states\")\n",
    "        print(\"Curr_Traj_shape:\",Traj.shape)\n",
    "        #Sampling the accumulated trajectories\n",
    "        Sampling_Indeces=jax.random.permutation(random.PRNGKey(961*(i+1)), jnp.arange(0,Traj.shape[0],1)) \n",
    "        Traj=Traj[Sampling_Indeces[:min(Max_dataset_size,Traj.shape[0])],...]\n",
    "        Train,Val,Test, shift_fn,Batch_pair_cutoffs,Batch_pair_sigma,Batch_Disp_Vec_fn,Batch_Node_energy_fn,Batch_Total_energy_fn,displacement_fn, shift_fn,Disp_Vec_fn =Sys.create_batched_States(random.PRNGKey(147),System='LJ',spatial_dimension=3,N=100, N_sample =100,Batch_size=4,Traj=Traj)        \n",
    "    grads_acc=scalar_mult_grad(0.0,init_grad)\n",
    "    keys2=random.split(keys1[i],Batch_size)\n",
    "    \n",
    "    Batch_loss=0\n",
    "    for p in range(Batch_size):\n",
    "        ((loss_val,(Systate,log)),grads) = loss_grad_fn(params,Train[(Batch_size*i+p)%len(Train)],keys2[p],len_ep=len_ep)\n",
    "        print(\"Batch: \",p,' ',loss_val)\n",
    "        print_log(log,is_plot=False,epoch_id=i,Batch_id=p)\n",
    "        Batch_loss+=loss_val\n",
    "        grads_acc=add_grads(grads_acc,grads)\n",
    "        if(i>0):\n",
    "            if(jnp.sum(Train[(Batch_size*i+p)%len(Train)].pe)>jnp.sum(Systate.pe)):\n",
    "                    Traj=jnp.concatenate([Traj,Systate.R])\n",
    "    \n",
    "    if((i+1)%Val_freq==0):\n",
    "        Val_Batch_loss=0\n",
    "        for p in range(Batch_size):\n",
    "            ((val_loss_val,(val_Systate,val_log)),grads) = loss_grad_fn(params,Val[(Batch_size*i+p)%len(Val)],keys2[p],len_ep=20)\n",
    "            print_log(val_log,is_plot=False,epoch_id=i,Batch_id=p)\n",
    "            print(\"D_PE_sum:\",jnp.sum(val_Systate.pe-Val[(Batch_size*i+p)%len(Val)].pe))\n",
    "        \n",
    "            Val_loss_data+=[Val_Batch_loss/Batch_size]\n",
    "            Val_loss_epochs+=[i]\n",
    "            Val_Batch_loss+=val_loss_val\n",
    "        \n",
    "    updates, opt_state = tx.update(scalar_mult_grad(1/Batch_size,grads_acc), opt_state,params)\n",
    "    Train_loss_data+=[Batch_loss/Batch_size]\n",
    "    Train_loss_epochs+=[i]\n",
    "    #old_params=params\n",
    "    #print(\"myGrads:\",scalar_mult_grad(1/Batch_size,grads_acc))\n",
    "    #print(\"Before update\",params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    #print(\"After_update\",params)\n",
    "    #print(jax.tree_multimap(lambda x,y:100*(x-y)/y,params,old_params))\n",
    "    if i % Print_freq == 0:\n",
    "        if((i+1)%Val_freq==0):\n",
    "            print('Val-Loss step {}: '.format(i), Val_Batch_loss/Batch_size)\n",
    "        print('Loss step {}:{} '.format(i, Batch_loss/Batch_size))\n",
    "        #print('P.E {}: '.format(Systate.pe))\n",
    "        #fig,ax=plt.subplots(1,1,figsize=(10,10))\n",
    "        #Sys.plot_frame_edge(ax,Systate)\n",
    "        #fig.savefig(\"./Plots/System_\"+str(i)+\"_\"+str(Systate.pe)+\"_plot\"+\".png\")\n",
    "        #plt.close(fig)\n",
    "    if(i%Model_save_freq==0):\n",
    "        ckp.save_checkpoint(\"./checkpoints/\",params,i,overwrite=True,keep=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 The Learn2hop Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Re-creation of jax_md/nn.py to allow for vmap-enabled functions.\n",
    "\n",
    "This file replicates jax_md/nn.py which contains neural network primatives, but\n",
    "extends a number of the functions in order to allow them be vmapped. For\n",
    "example, the current implementation of the radial basis functions in nn.py\n",
    "uses a gather of species indexes which does not compose with vmaps. To get\n",
    "this functionality, we reproduce the code using masks.\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def _behler_parrinello_cutoff_fn(dr, cutoff_distance = 8.0):\n",
    "    \"\"\"Function of pairwise distance that smoothly goes to zero at the cutoff.\"\"\"\n",
    "    return jnp.where((dr < cutoff_distance) & (dr > 1e-7),0.5 * (jnp.cos(np.pi * dr / cutoff_distance) + 1), 0)\n",
    "\n",
    "\n",
    "def radial_symmetry_functions(metric_mapped,\n",
    "                              etas,\n",
    "                              cutoff_distance,\n",
    "                              num_species = 1):\n",
    "    \"\"\"Returns a function that computes radial symmetry functions.\n",
    "\n",
    "        This is a re-implementation of the radial symmetry functions within\n",
    "        nn.py but allows for vmapping by making use of input masking rather than\n",
    "        array indexing.\n",
    "\n",
    "        Args:\n",
    "        metric_mapped: displacement function that computes distances in\n",
    "          spatial positions between all pairs of atoms\n",
    "        etas: [num_etas], list corresponding to strength of interaction terms\n",
    "        cutoff_distance: neighbors whose distance is larger than cutoff_distance do\n",
    "          not contribute to each others symmetry functions. The contribution of a\n",
    "          neighbor to the symmetry function and its derivative goes to zero at this\n",
    "          distance. [0.0009, 0.01, 0.02, 0.035, 0.06, 0.1, 0.2, 0.4]\n",
    "        num_species: total number of species\n",
    "\n",
    "        Returns:\n",
    "        A function that computes the radial symmetry fucntions from inputs, yielding\n",
    "        output of shape [num_atoms,num_etas* num_species] to maintain the type\n",
    "        consistency from nn.py\n",
    "        \"\"\"\n",
    "    def radial_fn(eta, dr):\n",
    "        cutoffs = _behler_parrinello_cutoff_fn(dr, cutoff_distance)\n",
    "        return jnp.exp(-eta * dr**2) * cutoffs\n",
    "\n",
    "    #@jax.jit\n",
    "    def compute_fun(positions, species):\n",
    "        def return_radial(atom_type):\n",
    "            mask = species == atom_type\n",
    "            dr = metric_mapped(positions, positions)\n",
    "            radial = jax.vmap(radial_fn, (0, None))(etas, dr)\n",
    "            radial_masked = radial * mask.reshape([1, -1, 1])\n",
    "            return jnp.sum(radial_masked, axis=-1)\n",
    "\n",
    "        radial_vmap = jax.vmap(return_radial)\n",
    "        radial_symmetry = radial_vmap(np.arange(num_species))\n",
    "        radial_symmetry = jnp.transpose(radial_symmetry, (2, 0, 1))\n",
    "        return radial_symmetry.reshape([radial_symmetry.shape[0], -1])\n",
    "\n",
    "    return compute_fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(_behler_parrinello_cutoff_fn(jnp.arange(0.001,10,0.01), cutoff_distance = 8.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etas=jnp.array([0.001, 0.01 ,0.05, 0.1, 0.2, 0.4,1,2])\n",
    "radial_feats_fn=radial_symmetry_functions(pair_dist_fn,etas,2.5,1)\n",
    "radial_feats_fn=jax.jit(jax.vmap(radial_feats_fn,(0,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_dist_fn(R1,R2):\n",
    "    dR = Disp_Vec_fn(R1, R2)\n",
    "    dr = space.distance(dR)\n",
    "    return dr\n",
    "pair_dist_fn=jax.jit(pair_dist_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_R=Train[0].R[:]\n",
    "Test_species=jnp.zeros(Train[0].species[:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_dist_fn(Test_R,Test_R).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_feats=jax.jit(jax.vmap(radial_feats_fn,(0,0)))(Test_R,Test_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_feats[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_feats[:10,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_feats[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(radial_feats[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Training and Validation\n",
    "\n",
    "Train_loss_data   =  []\n",
    "Train_loss_epochs =  []\n",
    "Val_loss_data     =  []\n",
    "Val_loss_epochs   =  []\n",
    "Batch_size        =  3\n",
    "len_ep            =  10\n",
    "Max_dataset_size  =  1000\n",
    "Print_freq        =  1\n",
    "plot_freq         =  100\n",
    "Model_save_freq   =  10\n",
    "N_epoch           =  800\n",
    "Val_freq          =  20\n",
    "Reset_dataset_every= 1\n",
    "keys1=random.split(random.PRNGKey(117721817),N_epoch)\n",
    "for i in range(N_epoch):\n",
    "    if((i+1)%Reset_dataset_every==0):\n",
    "        print(\"Re creating dataset with new states\")\n",
    "        print(\"Curr_Traj_shape:\",Traj.shape)\n",
    "        #Sampling the accumulated trajectories\n",
    "        Sampling_Indeces=jax.random.permutation(random.PRNGKey(961*(i+1)), jnp.arange(0,Traj.shape[0],1)) \n",
    "        Traj=Traj[:min(Max_dataset_size,Traj.shape[0]),...]\n",
    "        Train,Val,Test, shift_fn,Batch_pair_cutoffs,Batch_pair_sigma,Batch_Disp_Vec_fn,Batch_Node_energy_fn,Batch_Total_energy_fn=Sys.create_batched_States(random.PRNGKey(147),System='LJ',spatial_dimension=3,N=100, N_sample =Traj.shape[0],Batch_size=4,Traj=Traj)        \n",
    "    grads_acc=scalar_mult_grad(0.0,init_grad)\n",
    "    keys2=random.split(keys1[i],Batch_size)\n",
    "    \n",
    "    Batch_loss=0\n",
    "    for p in range(Batch_size):\n",
    "        ((loss_val,(Systate,log)),grads) = loss_grad_fn(params,Train[(Batch_size*i+p)%len(Train)],keys2[p],len_ep=len_ep)\n",
    "        print(\"Batch: \",p,' ',loss_val)\n",
    "        print_log(log,is_plot=False,epoch_id=i,Batch_id=p)\n",
    "        Batch_loss+=loss_val\n",
    "        grads_acc=add_grads(grads_acc,grads)\n",
    "        if(i>0):\n",
    "            if(jnp.sum(Train[(Batch_size*i+p)%len(Train)].pe)<jnp.sum(Systate.pe)):\n",
    "                    Traj=jnp.concatenate([Traj,Systate.R])\n",
    "    \n",
    "    if((i+1)%Val_freq==0):\n",
    "        Val_Batch_loss=0\n",
    "        for p in range(Batch_size):\n",
    "            ((val_loss_val,(val_Systate,val_log)),grads) = loss_grad_fn(params,Val[(Batch_size*i+p)%len(Val)],keys2[p],len_ep=20)\n",
    "            print_log(val_log,is_plot=False,epoch_id=i,Batch_id=p)\n",
    "            print(\"D_PE_sum:\",jnp.sum(val_Systate.pe-Val[(Batch_size*i+p)%len(Val)].pe))\n",
    "        \n",
    "            Val_loss_data+=[Val_Batch_loss/Batch_size]\n",
    "            Val_loss_epochs+=[i]\n",
    "            Val_Batch_loss+=val_loss_val\n",
    "        \n",
    "    updates, opt_state = tx.update(scalar_mult_grad(1/Batch_size,grads_acc), opt_state,params)\n",
    "    Train_loss_data+=[Batch_loss/Batch_size]\n",
    "    Train_loss_epochs+=[i]\n",
    "    #old_params=params\n",
    "    #print(\"myGrads:\",scalar_mult_grad(1/Batch_size,grads_acc))\n",
    "    #print(\"Before update\",params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    #print(\"After_update\",params)\n",
    "    #print(jax.tree_multimap(lambda x,y:100*(x-y)/y,params,old_params))\n",
    "    if i % Print_freq == 0:\n",
    "        if((i+1)%Val_freq==0):\n",
    "            print('Val-Loss step {}: '.format(i), Val_Batch_loss/Batch_size)\n",
    "        print('Loss step {}:{} '.format(i, Batch_loss/Batch_size))\n",
    "        #print('P.E {}: '.format(Systate.pe))\n",
    "        #fig,ax=plt.subplots(1,1,figsize=(10,10))\n",
    "        #Sys.plot_frame_edge(ax,Systate)\n",
    "        #fig.savefig(\"./Plots/System_\"+str(i)+\"_\"+str(Systate.pe)+\"_plot\"+\".png\")\n",
    "        #plt.close(fig)\n",
    "    if(i%Model_save_freq==0):\n",
    "        ckp.save_checkpoint(\"./checkpoints/\",params,i,overwrite=True,keep=400)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
